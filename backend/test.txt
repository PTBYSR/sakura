#%%
from langchain.chat_models import init_chat_model
from langgraph.graph import StateGraph, START
from langgraph.graph.message import add_messages
from langchain_core.tools import tool
from langgraph.prebuilt import ToolNode, tools_condition
from langchain.schema import Document
from typing import Annotated
from typing_extensions import TypedDict
from pathlib import Path
import jsonlines
from langchain.docstore.document import Document
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from dotenv import load_dotenv
from langsmith import traceable
from tqdm import tqdm  # nice progress bar
import re

#%%
from dotenv import load_dotenv

load_dotenv()
#%%
class State(TypedDict):



    messages: Annotated[list, add_messages]
#%%
@tool
def get_stock_price(symbol: str) -> float:
    """
    Get the current stock price for a given stock symbol.

    This function returns mock stock prices for demonstration purposes.
    Supports major US stocks (MSFT, AAPL, AMZN) and Indian stocks (RIL).

    :param symbol: Stock symbol/ticker (e.g., 'MSFT', 'AAPL', 'AMZN', 'RIL')
    :type symbol: str
    :return: Current stock price in USD (or local currency for non-US stocks)
    :rtype: float
    :raises: None - returns 0.0 for unknown symbols

    Example:
        >>> get_stock_price("MSFT")
        200.3
        >>> get_stock_price("INVALID")
        0.0
    """
    return {
        "MSFT": 200.3,
        "AAPL": 200.3,
        "AMZN": 200.3,
        "RIL": 87.6,
    }.get(symbol, 0.0)


print("loaded stock price about to enter kb loading")
#%%
kb_folder = Path("jsonl_data")
index_dir = Path("faiss_index")  


docs = []
embeddings = HuggingFaceEmbeddings()
#%%
# for file in kb_folder.glob("*.jsonl"):
#     with jsonlines.open(file) as reader:
#         for obj in reader:
#             docs.append(Document(page_content=obj["text"], metadata={"id": obj["id"]}))

# --- Load existing FAISS index if available ---
if index_dir.exists():
    print("ðŸ”„ Loading cached FAISS index...")
    vector_store = FAISS.load_local(
        str(index_dir),
        embeddings,
        allow_dangerous_deserialization=True
    )
else:
    print("âš¡ Building FAISS index from JSONL files...")
    for file in kb_folder.glob("*.jsonl"):
        with jsonlines.open(file) as reader:
            for obj in reader:
                docs.append(Document(
                    page_content=obj["text"],
                    metadata={"id": obj["id"]}
                ))

    vector_store = FAISS.from_documents(docs, embeddings)
    vector_store.save_local(str(index_dir))
    print("âœ… FAISS index built and saved.")




@tool
def retrieve_from_kb(query: str, top_k: int = 3) -> str:
    """
    Retrieve top-k relevant KB entries for a user query.
    """
    results = vector_store.similarity_search(query, k=top_k)
    return "\n".join([r.page_content for r in results])
#%%

tools = [get_stock_price, retrieve_from_kb]

llm = init_chat_model("google_genai:gemini-2.0-flash")
llm_with_tools = llm.bind_tools(tools)

def chatbot(state: State):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}

builder = StateGraph(State)

builder.add_node(chatbot)
builder.add_node("tools", ToolNode(tools))

builder.add_edge(START, "chatbot")
builder.add_conditional_edges("chatbot", tools_condition)
builder.add_edge("tools", "chatbot")

graph = builder.compile()


from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))


#%%
system_message = {
    "role": "system",
    "content": (
        "You are a helpful and friendly customer support assistant for Regirl, "
        "a wig-selling company. Always assume the customer is asking about Regirl's "
        "products, policies, and shipping unless otherwise specified."
    )
}

#%%
@traceable
def call_graph(query: str) -> str:
    """
    Sends a query to the LangGraph chatbot with system context for Regirl,
    retrieves the last message, and formats it for cleaner display.
    """
    messages = [
        system_message,  # <-- system prompt here
        {"role": "user", "content": query}
    ]

    state = graph.invoke({"messages": messages})
    raw_response = state["messages"][-1].content

    # Clean formatting
    formatted = raw_response.replace("\\n", "\n")
    formatted = re.sub(r"\*\*|\*", "", formatted)
    formatted = "\n".join(line.strip() for line in formatted.split("\n") if line.strip())

    return formatted

#%%
# Get user input
user_query = input("Enter your question: ")

# Call the graph with the input
answer = call_graph(user_query)

# Print the response
print(answer)
